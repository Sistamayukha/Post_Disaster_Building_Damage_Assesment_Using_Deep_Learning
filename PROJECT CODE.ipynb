{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting zipped files"
      ],
      "metadata": {
        "id": "XviCxhJ1rToN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/691_Team4_Dataset/Dataset.zip\"\n",
        "extract_to = \"/content/Dataset\"\n",
        "\n",
        "# Unzip only if not already extracted\n",
        "if not os.path.exists(extract_to):\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"‚úÖ Extraction complete.\")\n",
        "else:\n",
        "    print(\"‚úÖ Already extracted.\")"
      ],
      "metadata": {
        "id": "yyzs4wfHoxNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geopandas rasterio shapely"
      ],
      "metadata": {
        "id": "k5ZBLkG0pPCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely import wkt\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "from glob import glob\n",
        "import random"
      ],
      "metadata": {
        "id": "LUbZPsxUoxKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assigning Labels and Colours to classes"
      ],
      "metadata": {
        "id": "1VBq6mXmraYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "# xBD damage label mapping\n",
        "LABEL_NAME_TO_NUM = {\n",
        "    'no-damage': 1,\n",
        "    'minor-damage': 2,\n",
        "    'major-damage': 3,\n",
        "    'destroyed': 4,\n",
        "    'un-classified': 5\n",
        "}\n",
        "\n",
        "# Assign RGB colors to each class (visual)\n",
        "CLASS_COLORS = {\n",
        "    1: (0, 255, 0),      # no-damage: green\n",
        "    2: (255, 255, 0),    # minor: yellow\n",
        "    3: (255, 165, 0),    # major: orange\n",
        "    4: (255, 0, 0),      # destroyed: red\n",
        "    5: (128, 128, 128),  # unclassified: gray\n",
        "}"
      ],
      "metadata": {
        "id": "OG6F_ZVPoxIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Mask Based on Damage Labels"
      ],
      "metadata": {
        "id": "F9YzkCe5rfMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(image_shape, label_json):\n",
        "    height, width = image_shape # Extracting height and width from the image shape\n",
        "    mask = np.zeros((height, width), dtype=np.uint8) # Initializing a mask with zeros (black)\n",
        "\n",
        "    for feat in label_json['features']['xy']:\n",
        "        damage = feat['properties'].get('subtype', 'no-damage')# Get the damage subtype\n",
        "        label = LABEL_NAME_TO_NUM.get(damage, 1) # Mapping the damage subtype to a numeric label\n",
        "        polygon = wkt.loads(feat['wkt'])\n",
        "        coords = np.array(polygon.exterior.coords, np.int32)# Getting the coordinates of the polygon's exterior\n",
        "\n",
        "        # Filling the polygon area in the mask with the corresponding label\n",
        "        cv2.fillPoly(mask, [coords], label)\n",
        "    return mask  # Return the final mask with filled polygons corresponding to the damage regions"
      ],
      "metadata": {
        "id": "7QWLgefboxFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overlay Mask on Image for Visualization"
      ],
      "metadata": {
        "id": "4pNHx-8lrlkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_mask(image_path, label_json_path, title=\"\"):\n",
        "    with open(label_json_path) as f:\n",
        "        label_json = json.load(f)\n",
        "\n",
        "    with rasterio.open(image_path) as src:\n",
        "        img = src.read([1, 2, 3]).transpose(1, 2, 0)  # RGB\n",
        "\n",
        "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "    mask = create_mask(img.shape[:2], label_json)\n",
        "\n",
        "    color_mask = np.zeros_like(img)\n",
        "    for val, color in CLASS_COLORS.items():\n",
        "        color_mask[mask == val] = color\n",
        "\n",
        "    overlay = cv2.addWeighted(img, 0.6, color_mask, 0.4, 0)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(overlay)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MYvdKQsLoxDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "3G7MWBuf3kol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#File Paths and Sample Selection for Pre-Disaster Labels and Images"
      ],
      "metadata": {
        "id": "wWVk9u_Yrq1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dir = \"/content/Dataset/Dataset/train/hurricane/labels\"\n",
        "pre_img_dir = \"/content/Dataset/Dataset/train/hurricane/images\"\n",
        "post_img_dir = \"/content/Dataset/Dataset/train/hurricane/images\"\n",
        "label_files = sorted(glob(os.path.join(label_dir, \"*pre_disaster.json\")))\n",
        "sample_files = random.sample(label_files, 10) # Randomly select 10 label files for processing"
      ],
      "metadata": {
        "id": "hC7mQka1sZ0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Looping through each randomly sampled pre-disaster label file for visualization"
      ],
      "metadata": {
        "id": "VX5-7I6srwPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pre_label_path in sample_files:\n",
        "    base = os.path.basename(pre_label_path).replace(\"_pre_disaster.json\", \"\")\n",
        "    post_label_path = pre_label_path.replace(\"pre_disaster\", \"post_disaster\")\n",
        "\n",
        "    pre_image = os.path.join(pre_img_dir, f\"{base}_pre_disaster.tif\")\n",
        "    post_image = os.path.join(post_img_dir, f\"{base}_post_disaster.tif\")\n",
        "\n",
        "    print(f\"‚ñ∂Ô∏è Visualizing: {base}\")\n",
        "    overlay_mask(pre_image, pre_label_path, title=f\"{base} ‚Äî Pre-disaster (Building Mask)\")\n",
        "    overlay_mask(post_image, post_label_path, title=f\"{base} ‚Äî Post-disaster (Damage Mask)\")"
      ],
      "metadata": {
        "id": "QXD6MTPosZyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis and Visualization of Pre- and Post-Disaster Data: Area Calculations and Damage Class Distribution"
      ],
      "metadata": {
        "id": "PfDzGF_3r1me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from shapely import wkt\n",
        "from shapely.geometry import Polygon\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "\n",
        "# Paths\n",
        "label_dir = \"/content/Dataset/Dataset/train/hurricane/labels\"\n",
        "pre_label_files = sorted(glob(os.path.join(label_dir, \"*pre_disaster.json\"))) # List of pre-disaster label files (sorted)\n",
        "post_label_files = sorted(glob(os.path.join(label_dir, \"*post_disaster.json\"))) # List of post-disaster label files (sorted)\n",
        "\n",
        "# For bar chart 1: building vs non-building (pre-disaster)\n",
        "# Initializing lists to store areas of buildings and non-building regions\n",
        "building_areas = []\n",
        "non_building_areas = []\n",
        "\n",
        "# For bar chart 2: count of each damage class (post-disaster)\n",
        "# Initializing a dictionary to count the number of instances for each damage class (post-disaster)\n",
        "damage_class_counts = {\n",
        "    'no-damage': 0,\n",
        "    'minor-damage': 0,\n",
        "    'major-damage': 0,\n",
        "    'destroyed': 0,\n",
        "    'un-classified': 0\n",
        "}\n",
        "\n",
        "# Compute areas\n",
        "for pre_json_path in pre_label_files:\n",
        "    with open(pre_json_path) as f:\n",
        "        data = json.load(f)\n",
        "    polygons = [wkt.loads(feat['wkt']) for feat in data['features']['xy']]\n",
        "    area = sum(poly.area for poly in polygons if poly.is_valid)  # Compute the total area of valid polygons\n",
        "    building_areas.append(area) # Add the computed building area to the list\n",
        "    # Assume full tile is 1024x1024 = 1,048,576 pixels\n",
        "    non_building_areas.append(1024 * 1024 - area)\n",
        "\n",
        "# Count damage classes\n",
        "for post_json_path in post_label_files:\n",
        "    with open(post_json_path) as f:\n",
        "        data = json.load(f)\n",
        "    for feat in data['features']['xy']:\n",
        "        damage = feat['properties'].get('subtype', 'no-damage')\n",
        "        if damage in damage_class_counts: # If the damage type is one of the predefined categories\n",
        "            damage_class_counts[damage] += 1\n",
        "        else:\n",
        "            damage_class_counts['no-damage'] += 1  # default fallback\n",
        "\n",
        "# Plot building vs non-building area\n",
        "avg_building_area = np.mean(building_areas)\n",
        "avg_non_building_area = np.mean(non_building_areas)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar([\"Building\", \"Non-Building\"], [avg_building_area, avg_non_building_area], color=[\"green\", \"gray\"])\n",
        "plt.title(\"Average Area: Building vs Non-Building (Pre-disaster)\")\n",
        "plt.ylabel(\"Average Area (pixels)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot damage class distribution\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(damage_class_counts.keys(), damage_class_counts.values(), color=[\"green\", \"yellow\", \"orange\", \"red\", \"gray\"])\n",
        "plt.title(\"Count of Buildings per Damage Class (Post-disaster)\")\n",
        "plt.ylabel(\"Number of Buildings\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a74Ks0ySsZwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "9RB6KADKmiVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrame Creation and Seaborn Visualization: Plotting Average Building vs Non-Building Area\n"
      ],
      "metadata": {
        "id": "KkGT2-EBr8ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame for seaborn\n",
        "area_df = pd.DataFrame({\n",
        "    \"Category\": [\"Building\", \"Non-Building\"],\n",
        "    \"Average Area (pixels)\": [avg_building_area, avg_non_building_area]\n",
        "})\n",
        "\n",
        "# Seaborn bar plot with annotations\n",
        "plt.figure(figsize=(6, 4))\n",
        "ax1 = sns.barplot(data=area_df, x=\"Category\", y=\"Average Area (pixels)\", palette=[\"green\", \"gray\"])\n",
        "plt.title(\"Average Area: Building vs Non-Building (Pre-disaster)\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Add value annotations\n",
        "for p in ax1.patches:\n",
        "    height = p.get_height()\n",
        "    ax1.annotate(f'{height:,.0f}',\n",
        "                 (p.get_x() + p.get_width() / 2., height),\n",
        "                 ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GoyYXfzqmggc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting Damage Class Distribution"
      ],
      "metadata": {
        "id": "U4-t0pg1sAuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dict to DataFrame\n",
        "damage_df = pd.DataFrame(list(damage_class_counts.items()), columns=[\"Damage Class\", \"Count\"])\n",
        "\n",
        "# Color palette\n",
        "colors = {\n",
        "    \"no-damage\": \"green\",\n",
        "    \"minor-damage\": \"yellow\",\n",
        "    \"major-damage\": \"orange\",\n",
        "    \"destroyed\": \"red\",\n",
        "    \"un-classified\": \"gray\"\n",
        "}\n",
        "# Map the color palette to the \"Damage Class\" column in the DataFrame\n",
        "palette = [colors[d] for d in damage_df[\"Damage Class\"]]\n",
        "\n",
        "# Seaborn bar plot with annotations\n",
        "plt.figure(figsize=(8, 4))\n",
        "ax2 = sns.barplot(data=damage_df, x=\"Damage Class\", y=\"Count\", palette=palette)\n",
        "plt.title(\"Count of Buildings per Damage Class (Post-disaster)\")\n",
        "plt.ylabel(\"Number of Buildings\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.grid(True)\n",
        "\n",
        "# Add value annotations\n",
        "for p in ax2.patches:\n",
        "    height = p.get_height()\n",
        "    ax2.annotate(f'{height:,}',\n",
        "                 (p.get_x() + p.get_width() / 2., height),\n",
        "                 ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aVCHgyRAmwhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Mask Generation from Damage Labels: Polygon Drawing and Mask Saving"
      ],
      "metadata": {
        "id": "vTsg1WpnsE8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from shapely import wkt\n",
        "from shapely.geometry import Polygon, mapping\n",
        "from skimage.io import imread, imsave\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the class-to-index mapping\n",
        "LABEL_NAME_TO_NUM = {\n",
        "    'no-damage': 1,\n",
        "    'minor-damage': 2,\n",
        "    'major-damage': 3,\n",
        "    'destroyed': 4,\n",
        "    'un-classified': 5  # use 5 for buildings without damage labels\n",
        "}\n",
        "\n",
        "def get_image_shape(image_path):\n",
        "    img = imread(image_path)\n",
        "    return img.shape[:2]  # (H, W)\n",
        "\n",
        "def read_label_json(json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data['features']['xy']\n",
        "\n",
        "def get_polygons_and_labels(features):\n",
        "    polygons = []\n",
        "    for feat in features:\n",
        "        wkt_poly = feat['wkt']\n",
        "        damage = feat['properties'].get('subtype', 'no-damage')\n",
        "        label = LABEL_NAME_TO_NUM.get(damage, 1)  # default to 1 (no-damage)\n",
        "        polygon = wkt.loads(wkt_poly)\n",
        "        polygons.append((np.array(polygon.exterior.coords, dtype=np.int32), label))\n",
        "    return polygons\n",
        "\n",
        "def draw_mask(shape, polygons, border_shrink=1):\n",
        "    mask = np.zeros(shape, dtype=np.uint8)\n",
        "\n",
        "    for coords, label in polygons:\n",
        "        polygon = Polygon(coords)\n",
        "        centroid_x, centroid_y = polygon.centroid.coords[0]\n",
        "        shrunk_coords = []\n",
        "        for x, y in polygon.exterior.coords:\n",
        "            x = x + border_shrink if x < centroid_x else x - border_shrink\n",
        "            y = y + border_shrink if y < centroid_y else y - border_shrink\n",
        "            shrunk_coords.append([x, y])\n",
        "        shrunk_coords = np.array(shrunk_coords, dtype=np.int32)\n",
        "        cv2.fillPoly(mask, [shrunk_coords], label)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def generate_mask(image_path, json_path, output_path, border_shrink=1):\n",
        "    print(f\"Processing: {os.path.basename(image_path)}\")\n",
        "    shape = get_image_shape(image_path)\n",
        "    features = read_label_json(json_path)\n",
        "    polygons = get_polygons_and_labels(features)\n",
        "    mask = draw_mask(shape, polygons, border_shrink)\n",
        "    imsave(output_path, mask)"
      ],
      "metadata": {
        "id": "wgnNWbTEtl3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mask Generation for Hurricane Dataset: Processing Images and Generating Masks"
      ],
      "metadata": {
        "id": "Fe6q0X5esJKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directories for images, labels, and output masks\n",
        "image_dir = \"/content/Dataset/Dataset/train/hurricane/images\"\n",
        "label_dir = \"/content/Dataset/Dataset/train/hurricane/labels\"\n",
        "output_dir = \"/content/masks\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Iterate over each file in the image directory\n",
        "for filename in tqdm(os.listdir(image_dir)):\n",
        "    if not filename.endswith(\".tif\"):\n",
        "        continue\n",
        "    base_name = filename.replace(\".tif\", \"\")\n",
        "    # Construct the full paths for the image, label JSON, and output mask\n",
        "    image_path = os.path.join(image_dir, filename)\n",
        "    json_path = os.path.join(label_dir, f\"{base_name}.json\")\n",
        "    output_path = os.path.join(output_dir, f\"{base_name}_mask.png\")\n",
        "\n",
        "    if os.path.exists(json_path):\n",
        "        generate_mask(image_path, json_path, output_path, border_shrink=1)\n",
        "    else:\n",
        "        print(f\"Missing label for {filename}\")"
      ],
      "metadata": {
        "id": "kcbxQi6_r9xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Post-Disaster Satellite Image with Damage Class Overlay"
      ],
      "metadata": {
        "id": "QQqnAHPBsTjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "\n",
        "# Color palette for each damage class (1‚Äì5)\n",
        "CLASS_COLORS = {\n",
        "    1: (0, 255, 0),     # no-damage - green\n",
        "    2: (255, 255, 0),   # minor-damage - yellow\n",
        "    3: (255, 165, 0),   # major-damage - orange\n",
        "    4: (255, 0, 0),     # destroyed - red\n",
        "    5: (128, 128, 128)  # unclassified - gray\n",
        "}\n",
        "\n",
        "# visualize the post-disaster image with an overlay of the damage classes\n",
        "def visualize_post_image_with_mask(image_path, mask_path, title=\"\"):\n",
        "    # Load RGB satellite image\n",
        "    with rasterio.open(image_path) as src:\n",
        "        img = src.read([1, 2, 3]).transpose(1, 2, 0) # Read the 3 RGB channels (Red, Green, Blue)\n",
        "        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8) # Normalize the image to [0, 255]\n",
        "\n",
        "\n",
        "    # Load grayscale mask image\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Initialize an empty mask image for colored overlay\n",
        "    color_mask = np.zeros_like(img)\n",
        "\n",
        " # Iterate over each damage class (1-5) and assign the corresponding color from CLASS_COLORS\n",
        "    for cls, color in CLASS_COLORS.items():\n",
        "        color_mask[mask == cls] = color\n",
        "\n",
        " # Overlay the color mask on the original image with a weight of 0.7 for the image and 0.3 for the mask\n",
        "    overlay = cv2.addWeighted(img, 0.7, color_mask, 0.3, 0)\n",
        "\n",
        "    # Show side-by-side\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6)) # Create two subplots (side by side)\n",
        "    axs[0].imshow(img)\n",
        "    axs[0].set_title(\"Post-Disaster Image\")\n",
        "    axs[1].imshow(overlay)\n",
        "    axs[1].set_title(\"Overlay with Damage Classes\")\n",
        "    for ax in axs:\n",
        "        ax.axis(\"off\")\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "a_0I-Jzsr9uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Post-Disaster Images with Corresponding Damage Masks"
      ],
      "metadata": {
        "id": "mim8b2Umtnsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to post-disaster images and masks\n",
        "image_dir = \"/content/Dataset/Dataset/train/hurricane/images\"\n",
        "mask_dir = \"/content/masks\"\n",
        "\n",
        "# Pick only post-disaster images\n",
        "post_images = sorted(glob(os.path.join(image_dir, \"*post_disaster.tif\")))\n",
        "post_image_samples = post_images[:15]\n",
        "\n",
        "# Visualize each with its corresponding mask\n",
        "for image_path in post_image_samples:\n",
        "   # Extract the base name (without extension) for matching mask files\n",
        "    base_name = os.path.basename(image_path).replace(\".tif\", \"\")\n",
        "    mask_path = os.path.join(mask_dir, f\"{base_name}_mask.png\")\n",
        "    if os.path.exists(mask_path):\n",
        "        visualize_post_image_with_mask(image_path, mask_path, title=base_name)\n",
        "    else:\n",
        "        print(f\"‚ùå Missing mask for: {base_name}\")"
      ],
      "metadata": {
        "id": "iaL4VBS-r9pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tile Generation for Satellite Images: Pre-disaster and Post-disaster Image Splitting with Damage and Building Masks"
      ],
      "metadata": {
        "id": "1GLeBe50tyZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tifffile as tiff\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_output_dirs(base_dir):\n",
        "    for sub in ['images_pre', 'images_post', 'damage_masks', 'building_masks']:\n",
        "        os.makedirs(os.path.join(base_dir, sub), exist_ok=True)\n",
        "\n",
        "def get_base_ids_from_paths(pre_image_paths):\n",
        "    return [os.path.basename(f).replace(\"_pre_disaster.tif\", \"\") for f in pre_image_paths]\n",
        "\n",
        "# This ensures the images are consistent for processing and visualization\n",
        "def convert_to_rgb(img):\n",
        "    if img.dtype != np.uint8:\n",
        "        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        img = img.astype(np.uint8)\n",
        "    if len(img.shape) == 2:\n",
        "        return np.stack([img] * 3, axis=-1)\n",
        "    if img.shape[2] >= 3:\n",
        "        return img[:, :, :3]\n",
        "    raise ValueError(f\"Unsupported image shape: {img.shape}, dtype: {img.dtype}\")\n",
        "\n",
        "# Main function to create image tiles and corresponding damage and building masks\n",
        "def create_tiles(image_dir, mask_dir, output_dir, tile_size=256, visualize_first=True):\n",
        "    make_output_dirs(output_dir)\n",
        "\n",
        "    pre_image_paths = sorted(glob(os.path.join(image_dir, \"*_pre_disaster.tif\")))\n",
        "    base_ids = get_base_ids_from_paths(pre_image_paths)\n",
        "\n",
        " # Iterate over each base ID (representing a disaster event)\n",
        "    for base in tqdm(base_ids, desc=\"Creating tiles\"):\n",
        "        pre_path = os.path.join(image_dir, f\"{base}_pre_disaster.tif\")\n",
        "        post_path = os.path.join(image_dir, f\"{base}_post_disaster.tif\")\n",
        "        dmg_path = os.path.join(mask_dir, f\"{base}_post_disaster_mask.png\")\n",
        "\n",
        "        if not (os.path.exists(pre_path) and os.path.exists(post_path) and os.path.exists(dmg_path)):\n",
        "            continue\n",
        "\n",
        "        pre_raw = tiff.imread(pre_path)\n",
        "        post_raw = tiff.imread(post_path)\n",
        "\n",
        "        pre_rgb = convert_to_rgb(pre_raw)\n",
        "        post_rgb = convert_to_rgb(post_raw)\n",
        "\n",
        "        damage_mask = cv2.imread(dmg_path, cv2.IMREAD_GRAYSCALE)\n",
        "        building_mask = (damage_mask > 0).astype(np.uint8) * 255\n",
        "\n",
        "        h, w = damage_mask.shape\n",
        "        count = 0\n",
        "\n",
        " # Iterate through the image and generate tiles (patches) of the specified size\n",
        "        for y in range(0, h, tile_size):\n",
        "            for x in range(0, w, tile_size):\n",
        "                if y + tile_size > h or x + tile_size > w:\n",
        "                    continue\n",
        "\n",
        " # Extract image patches for the pre-disaster, post-disaster, damage mask, and building mask\n",
        "                pre_patch = pre_rgb[y:y+tile_size, x:x+tile_size]\n",
        "                post_patch = post_rgb[y:y+tile_size, x:x+tile_size]\n",
        "                dmg_patch = damage_mask[y:y+tile_size, x:x+tile_size]\n",
        "                bld_patch = building_mask[y:y+tile_size, x:x+tile_size]\n",
        "\n",
        "                patch_id = f\"{base}_{y}_{x}_{count}\"# Generate a unique ID for each patch\n",
        "                cv2.imwrite(os.path.join(output_dir, \"images_pre\", f\"{patch_id}.png\"), pre_patch)\n",
        "                cv2.imwrite(os.path.join(output_dir, \"images_post\", f\"{patch_id}.png\"), post_patch)\n",
        "                cv2.imwrite(os.path.join(output_dir, \"damage_masks\", f\"{patch_id}.png\"), dmg_patch)\n",
        "                cv2.imwrite(os.path.join(output_dir, \"building_masks\", f\"{patch_id}.png\"), bld_patch)"
      ],
      "metadata": {
        "id": "y8YkEzDYwZ9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing Tile Generation for Satellite Images: Pre-disaster and Post-disaster Image Splitting with Damage and Building Masks"
      ],
      "metadata": {
        "id": "Lc2E0hfA1EYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_tiles(\n",
        "    image_dir=\"/content/Dataset/Dataset/train/hurricane/images\",\n",
        "    mask_dir=\"/content/masks\",\n",
        "    output_dir=\"/content/tiles\"\n",
        ")"
      ],
      "metadata": {
        "id": "9uBT7PrFwZ6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Satellite Image Tiles and Corresponding Masks: Pre-image, Post-image, Building Mask, and Damage Mask\n"
      ],
      "metadata": {
        "id": "eGKinHMx1TV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# Path to tile directory\n",
        "tile_dir = \"/content/tiles\"  # <-- update if needed\n",
        "\n",
        "# Get sample file names from images_pre\n",
        "sample_paths = sorted(glob(os.path.join(tile_dir, \"images_pre\", \"*.png\")))[:5]  # Visualize 5 tiles\n",
        "\n",
        "# Plotting\n",
        "n = len(sample_paths) # Number of samples to plot\n",
        "fig, axs = plt.subplots(n, 4, figsize=(12, 3 * n)) # Create subplots with 4 columns (pre, post, building mask, damage mask)\n",
        "\n",
        "# Iterate over each tile for visualization\n",
        "for i, pre_path in enumerate(sample_paths):\n",
        "    base_name = os.path.basename(pre_path)\n",
        "\n",
        "    # Construct paths for the corresponding post-image, building mask, and damage mask\n",
        "    post_path = os.path.join(tile_dir, \"images_post\", base_name)\n",
        "    bld_path = os.path.join(tile_dir, \"building_masks\", base_name)\n",
        "    dmg_path = os.path.join(tile_dir, \"damage_masks\", base_name)\n",
        "\n",
        "    # Load the pre-image, post-image, building mask, and damage mask\n",
        "    pre = cv2.imread(pre_path)\n",
        "    post = cv2.imread(post_path)\n",
        "    bld = cv2.imread(bld_path, cv2.IMREAD_GRAYSCALE)\n",
        "    dmg = cv2.imread(dmg_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Plot the images and masks in the appropriate subplot axes\n",
        "    axs[i, 0].imshow(cv2.cvtColor(pre, cv2.COLOR_BGR2RGB))\n",
        "    axs[i, 0].set_title(\"Pre Image\")\n",
        "\n",
        "    axs[i, 1].imshow(cv2.cvtColor(post, cv2.COLOR_BGR2RGB))\n",
        "    axs[i, 1].set_title(\"Post Image\")\n",
        "\n",
        "    axs[i, 2].imshow(bld, cmap=\"gray\")\n",
        "    axs[i, 2].set_title(\"Building Mask\")\n",
        "\n",
        "    axs[i, 3].imshow(dmg, cmap=\"nipy_spectral\", vmin=0, vmax=4)\n",
        "    axs[i, 3].set_title(\"Damage Mask\")\n",
        "\n",
        "    for ax in axs[i]:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TEbUFKFiiZM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "\n",
        "# source_folder = \"/content/shards\"\n",
        "# zip_name = \"/content/shards\"  # No .zip extension here\n",
        "\n",
        "# # This creates /content/Dataset_backup.zip\n",
        "# shutil.make_archive(zip_name, 'zip', source_folder)\n",
        "# print(\"‚úÖ Folder zipped successfully.\")\n"
      ],
      "metadata": {
        "id": "k_5QmDZXHBv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "\n",
        "# # Destination in Google Drive\n",
        "# destination_path = \"/content/final_splits_sliced.json\"\n",
        "\n",
        "# # Copy the zip\n",
        "# shutil.copy(\"/content/drive/MyDrive/691_Team4_Dataset/final_splits_sliced.json\", destination_path)\n",
        "# print(\"‚úÖ Zip file copied to Drive.\")"
      ],
      "metadata": {
        "id": "J4FYJ7QRHLHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the Disk Usage of the \"tiles\" Directory to Monitor Storage Consumption"
      ],
      "metadata": {
        "id": "X6sy9j7o6jbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !du -sh \"/content/drive/MyDrive/691_Team4_Dataset/shards.zip\"\n",
        "!du -sh \"/content/tiles\""
      ],
      "metadata": {
        "id": "3rlywFNluVEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting PNG Files Recursively in Directories: Images and Masks"
      ],
      "metadata": {
        "id": "v1H-YRX86p5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_post_count = 0\n",
        "for root, dirs, files in os.walk(\"/content/tiles/images_post\"):\n",
        "    images_post_count += sum(1 for f in files if f.endswith('.png'))\n",
        "\n",
        "print(\"Total files (recursive):\", images_post_count)\n",
        "\n",
        "images_pre_count = 0\n",
        "for root, dirs, files in os.walk(\"/content/tiles/images_pre\"):\n",
        "    images_pre_count += sum(1 for f in files if f.endswith('.png'))\n",
        "\n",
        "print(\"Total files (recursive):\", images_pre_count)\n",
        "\n",
        "building_masks_count = 0\n",
        "for root, dirs, files in os.walk(\"/content/tiles/building_masks\"):\n",
        "    building_masks_count += sum(1 for f in files if f.endswith('.png'))\n",
        "\n",
        "print(\"Total files (recursive):\", building_masks_count)\n",
        "\n",
        "damage_masks_count = 0\n",
        "for root, dirs, files in os.walk(\"/content/tiles/damage_masks\"):\n",
        "    damage_masks_count += sum(1 for f in files if f.endswith('.png'))\n",
        "\n",
        "print(\"Total files (recursive):\", damage_masks_count)"
      ],
      "metadata": {
        "id": "3OPXdpq-uVCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating .npz Shards for Image Tiles: Pre- and Post-Disaster Images, Masks, and Validations"
      ],
      "metadata": {
        "id": "WOLrDhOM7TOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_shards(tile_root, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get sorted list of pre-disaster image file paths (PNG files)\n",
        "    pre_image_paths = sorted(glob(os.path.join(tile_root, \"images_pre\", \"*.png\")))\n",
        "    skipped_log = []\n",
        "\n",
        "    # Iterate over each pre-disaster image file\n",
        "    for pre_path in tqdm(pre_image_paths, desc=\"Creating individual .npz shards\"):\n",
        "        try:\n",
        "            fname = os.path.basename(pre_path)\n",
        "            base_id = fname.replace(\".png\", \"\")\n",
        "\n",
        "            post_path = os.path.join(tile_root, \"images_post\", fname)\n",
        "            bld_path = os.path.join(tile_root, \"building_masks\", fname)\n",
        "            dmg_path = os.path.join(tile_root, \"damage_masks\", fname)\n",
        "\n",
        "            # Read all components\n",
        "            pre = cv2.imread(pre_path)\n",
        "            post = cv2.imread(post_path)\n",
        "            bld = cv2.imread(bld_path, cv2.IMREAD_GRAYSCALE)\n",
        "            dmg = cv2.imread(dmg_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Validations\n",
        "            if pre is None or post is None or bld is None or dmg is None:\n",
        "                raise ValueError(\"One or more files could not be read.\")\n",
        "\n",
        "            # Ensure that pre and post images are RGB (3-channel)\n",
        "            if pre.ndim != 3 or post.ndim != 3 or pre.shape[2] != 3 or post.shape[2] != 3:\n",
        "                raise ValueError(\"Pre/Post images are not RGB.\")\n",
        "\n",
        "            if pre.shape != post.shape or pre.shape[:2] != bld.shape or pre.shape[:2] != dmg.shape:\n",
        "                raise ValueError(\"Shape mismatch among components.\")\n",
        "\n",
        "            # Save the images and masks as a compressed .npz file\n",
        "            np.savez_compressed(\n",
        "                os.path.join(output_dir, f\"{base_id}.npz\"),\n",
        "                pre=pre.astype(np.uint8),\n",
        "                post=post.astype(np.uint8),\n",
        "                mask=dmg.astype(np.uint8),\n",
        "                bld_mask=bld.astype(np.uint8),\n",
        "                original_image_name=base_id\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            skipped_log.append(f\"{fname}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Save skipped log\n",
        "    if skipped_log:\n",
        "        with open(os.path.join(output_dir, \"skipped_tiles_log.txt\"), \"w\") as f:\n",
        "            for entry in skipped_log:\n",
        "                f.write(entry + \"\\n\")"
      ],
      "metadata": {
        "id": "oT52N4TC8Ffp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing .npz Shard Creation for Image Tiles: Pre- and Post-Disaster Images, Masks, and Validations\n"
      ],
      "metadata": {
        "id": "ho8qdqTj7uxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_shards(\n",
        "    tile_root=\"/content/tiles\",\n",
        "    output_dir=\"/content/shards\"\n",
        ")"
      ],
      "metadata": {
        "id": "aqDhBETuJCC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Split for .npz Files: Train, Validation, and Test Set Assignment\n"
      ],
      "metadata": {
        "id": "A2kPNT5v8BP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def create_data_split(npz_dir, output_json_path, seed=42, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "   # Ensure the ratios sum to 1\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-5\n",
        "\n",
        "    npz_files = sorted(glob(os.path.join(npz_dir, \"*.npz\")))\n",
        "    print(f\"Found {len(npz_files)} .npz files\")\n",
        "\n",
        "    # Group by scene_id\n",
        "    scene_groups = defaultdict(list) # Use a defaultdict to store files grouped by scene_id\n",
        "    for f in npz_files:\n",
        "        fname = os.path.basename(f).replace(\".npz\", \"\")\n",
        "        scene_id = \"_\".join(fname.split(\"_\")[:2])  # e.g., hurricane-harvey_00000092\n",
        "        scene_groups[scene_id].append(fname)\n",
        "\n",
        "    scene_ids = sorted(scene_groups.keys())\n",
        "    random.seed(seed) # Set the random seed for reproducibility\n",
        "    random.shuffle(scene_ids)\n",
        "\n",
        "    # Calculate the number of scenes for training, validation, and testing\n",
        "    num_scenes = len(scene_ids)\n",
        "    num_train = int(train_ratio * num_scenes)\n",
        "    num_val = int(val_ratio * num_scenes)\n",
        "    num_test = num_scenes - num_train - num_val\n",
        "\n",
        "     # Assign scenes to train, validation, and test sets\n",
        "    train_scenes = scene_ids[:num_train]\n",
        "    val_scenes = scene_ids[num_train:num_train + num_val]\n",
        "    test_scenes = scene_ids[num_train + num_val:]\n",
        "\n",
        "\n",
        "    # Create a dictionary to hold the split information for each scene_id\n",
        "    split_dict = {}\n",
        "    for scene_id in scene_ids:\n",
        "        split_dict[scene_id] = {\n",
        "            \"train\": scene_groups[scene_id] if scene_id in train_scenes else [],\n",
        "            \"val\": scene_groups[scene_id] if scene_id in val_scenes else [],\n",
        "            \"test\": scene_groups[scene_id] if scene_id in test_scenes else [],\n",
        "        }\n",
        "\n",
        "    with open(output_json_path, \"w\") as f:\n",
        "        json.dump(split_dict, f, indent=4)\n",
        "\n",
        "    print(f\"‚úÖ Split saved to {output_json_path}\")\n",
        "    print(f\"üìä Scenes ‚Üí Train: {len(train_scenes)}, Val: {len(val_scenes)}, Test: {len(test_scenes)}\")"
      ],
      "metadata": {
        "id": "n15zNwIZar1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "create_data_split(\n",
        "    npz_dir=\"/content/shards\",\n",
        "    output_json_path=\"/content/final_splits_sliced.json\"\n",
        ")"
      ],
      "metadata": {
        "id": "urPUlEaZary8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute and Save Mean and Standard Deviation for Image Tiles: Pre-image Normalization and Statistics Calculation"
      ],
      "metadata": {
        "id": "F-o9x0gz9KzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_mean_std(tile_root, output_json):\n",
        "    tile_stats = {} # Dictionary to store the mean and standard deviation for each tile\n",
        "\n",
        "    tile_paths = sorted(glob(os.path.join(tile_root, \"images_pre\", \"*.png\")))\n",
        "\n",
        "    # Iterate through each image file\n",
        "    for path in tqdm(tile_paths, desc=\"Computing mean/std\"):\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            print(f\"Image could not be read: {path}\")\n",
        "            continue\n",
        "\n",
        "        img = img.astype(np.float32) / 255.0  # Normalize the image to the range [0, 1]\n",
        "        mean = np.mean(img, axis=(0, 1)).tolist() # Compute the mean for each channel (RGB)\n",
        "        std = np.std(img, axis=(0, 1)).tolist()   # Compute the standard deviation for each channel (RGB)\n",
        "\n",
        "        key = os.path.basename(path).replace(\".png\", \"\")\n",
        "        tile_stats[key] = [mean, std] # Store the mean and std in the dictionary\n",
        "\n",
        "    with open(output_json, 'w') as f:\n",
        "        json.dump(tile_stats, f, indent=4)"
      ],
      "metadata": {
        "id": "HlKVQ6R3bbPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing mean and standard deviation"
      ],
      "metadata": {
        "id": "Ko1-XLV69kv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this\n",
        "compute_mean_std(\n",
        "    tile_root=\"/content/tiles\",\n",
        "    output_json=\"/content/mean_stddev_tiles.json\"\n",
        ")"
      ],
      "metadata": {
        "id": "x0I6w1C2arw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import json\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "y9rNe3E3fKyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dataset for Damage Detection: Handling Image Tiles, Augmentations and Normalization"
      ],
      "metadata": {
        "id": "aHArRE7U-EG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "from albumentations import Compose, HorizontalFlip, RandomRotate90\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\"\"\"\n",
        "        Initializes the dataset by setting parameters for tile IDs, base path,\n",
        "        and whether to apply transformations and normalization.\n",
        "\n",
        "        Args:\n",
        "            tile_ids (list): List of tile IDs (file names without extension).\n",
        "            mean_std_dict (dict): Dictionary containing the mean and std for each tile.\n",
        "            base_path (str): Base directory where the tile data is located.\n",
        "            transform (bool): Whether to apply augmentations.\n",
        "            normalize (bool): Whether to normalize the images.\n",
        "        \"\"\"\n",
        "class DamageDataset(Dataset):\n",
        "    def __init__(self, tile_ids, mean_std_dict, base_path, transform=True, normalize=True):\n",
        "        self.tile_ids = tile_ids\n",
        "        self.mean_std_dict = mean_std_dict\n",
        "        self.base_path = base_path\n",
        "        self.transform = transform\n",
        "        self.normalize = normalize\n",
        "\n",
        "         # Default fallback: ImageNet mean and std for normalization\n",
        "        self.default_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "        self.default_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "\n",
        "        # Augmentation\n",
        "        self.transform_aug = Compose([\n",
        "            HorizontalFlip(p=0.5),\n",
        "            RandomRotate90(p=0.5),\n",
        "            ToTensorV2()\n",
        "        ], additional_targets={'image1': 'image', 'mask1': 'mask'})\n",
        "\n",
        "         # Base transformation (no augmentation), only converts to tensor\n",
        "        self.transform_base = Compose([\n",
        "            ToTensorV2()\n",
        "        ], additional_targets={'image1': 'image', 'mask1': 'mask'})\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.tile_ids)  #Returns the number of samples in the dataset.\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tile_id = self.tile_ids[idx] #Fetches a sample from the dataset given an index.\n",
        "        npz_path = os.path.join(self.base_path, f\"{tile_id}.npz\")\n",
        "        if not os.path.exists(npz_path):\n",
        "            raise FileNotFoundError(f\"Missing: {npz_path}\")\n",
        "\n",
        "        data = np.load(npz_path)\n",
        "\n",
        "        pre = data[\"pre\"].astype(np.float32) / 255.0\n",
        "        post = data[\"post\"].astype(np.float32) / 255.0\n",
        "        bld = data[\"bld_mask\"]\n",
        "        dmg = data[\"mask\"]\n",
        "\n",
        "        # --- Mean/Std Handling ---\n",
        "        # Check if mean and std are available in the dictionary for the current tile_id\n",
        "        if tile_id in self.mean_std_dict:\n",
        "            try:\n",
        "                mean_list, std_list = self.mean_std_dict[tile_id]\n",
        "                mean = np.array(mean_list, dtype=np.float32)\n",
        "                std = np.array(std_list, dtype=np.float32)\n",
        "                std = np.where(std == 0, 1.0, std)  # prevent division by zero\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error in mean/std for {tile_id}: {e}\")\n",
        "                mean = self.default_mean\n",
        "                std = self.default_std\n",
        "        else:\n",
        "            print(f\"üîÅ Using default normalization for {tile_id}\")\n",
        "            mean = self.default_mean\n",
        "            std = self.default_std\n",
        "\n",
        "        if self.normalize:\n",
        "            pre = (pre - mean) / std\n",
        "            post = (post - mean) / std\n",
        "\n",
        "        # --- Transform ---\n",
        "         # Apply transformations: augmentation or base transformations\n",
        "        if self.transform:\n",
        "            transformed = self.transform_aug(image=pre, image1=post, mask=bld, mask1=dmg)\n",
        "        else:\n",
        "            transformed = self.transform_base(image=pre, image1=post, mask=bld, mask1=dmg)\n",
        "\n",
        "        # Return the transformed images and masks as a dictionary\n",
        "        return {\n",
        "            \"pre_image\": transformed['image'],\n",
        "            \"post_image\": transformed['image1'],\n",
        "            \"building_mask\": transformed['mask'].long(),\n",
        "            \"damage_mask\": transformed['mask1'].long()\n",
        "        }"
      ],
      "metadata": {
        "id": "Cl8olzXzxnXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation and Transformation for Image Tiles: Handling Train vs Test Transformations\n"
      ],
      "metadata": {
        "id": "7YiQWczl--sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train=True):\n",
        "  # If in training mode, apply augmentations such as random horizontal flip and random rotation\n",
        "    if train:\n",
        "        return A.Compose([  # Define a composition of augmentations\n",
        "            A.HorizontalFlip(p=0.5),  # Apply horizontal flip with a 50% probability\n",
        "            A.RandomRotate90(p=0.5),  # Apply random 90-degree rotation with a 50% probability\n",
        "            ToTensorV2() # Convert the image to a PyTorch tensor\n",
        "        ], additional_targets={\"image1\": \"image\"}) # Apply the same transformations to 'image1' (post-disaster)\n",
        "\n",
        "         # If in testing mode, only convert images to tensors (no augmentations)\n",
        "    else:\n",
        "        return A.Compose([  # Define a composition with only the tensor conversion\n",
        "            ToTensorV2()  # Convert the image to a PyTorch tensor\n",
        "        ], additional_targets={\"image1\": \"image\"})  # Apply the same transformation to 'image1' (post-disaster)"
      ],
      "metadata": {
        "id": "nGpMihA2xnVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "z7ih3xFmxnSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SiamUnet Architecture for Image Segmentation and Classification: Combining UNet with Siamese Networks\n"
      ],
      "metadata": {
        "id": "5YlucVjF_jZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiamUnet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels_s=2, out_channels_c=5, init_features=16):\n",
        "        super(SiamUnet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "\n",
        "        # UNet layers\n",
        "         # Encoder layers (downsampling)\n",
        "        self.encoder1 = SiamUnet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = SiamUnet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = SiamUnet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = SiamUnet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck layer\n",
        "        self.bottleneck = SiamUnet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        # --- Decoder layers (upsampling) ---\n",
        "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.decoder4 = SiamUnet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
        "        self.decoder3 = SiamUnet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
        "        self.decoder2 = SiamUnet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
        "        self.decoder1 = SiamUnet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        # Segmentation output layer\n",
        "        self.conv_s = nn.Conv2d(in_channels=features, out_channels=out_channels_s, kernel_size=1)\n",
        "\n",
        "        # Siamese classifier layers\n",
        "        # Similar convolution layers for classifying differences between two inputs\n",
        "        self.upconv4_c = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
        "        self.conv4_c = SiamUnet._block(features * 16, features * 16, name=\"conv4\")\n",
        "\n",
        "        self.upconv3_c = nn.ConvTranspose2d(features * 16, features * 4, kernel_size=2, stride=2)\n",
        "        self.conv3_c = SiamUnet._block(features * 8, features * 8, name=\"conv3\")\n",
        "\n",
        "        self.upconv2_c = nn.ConvTranspose2d(features * 8, features * 2, kernel_size=2, stride=2)\n",
        "        self.conv2_c = SiamUnet._block(features * 4, features * 4, name=\"conv2\")\n",
        "\n",
        "        self.upconv1_c = nn.ConvTranspose2d(features * 4, features, kernel_size=2, stride=2)\n",
        "        self.conv1_c = SiamUnet._block(features * 2, features * 2, name=\"conv1\")\n",
        "\n",
        "         # Final classification layer\n",
        "        self.conv_c = nn.Conv2d(in_channels=features * 2, out_channels=out_channels_c, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "\n",
        "        # UNet on x1\n",
        "        enc1_1 = self.encoder1(x1)\n",
        "        enc2_1 = self.encoder2(self.pool1(enc1_1))\n",
        "        enc3_1 = self.encoder3(self.pool2(enc2_1))\n",
        "        enc4_1 = self.encoder4(self.pool3(enc3_1))\n",
        "\n",
        "        bottleneck_1 = self.bottleneck(self.pool4(enc4_1))\n",
        "\n",
        "        dec4_1 = self.upconv4(bottleneck_1)\n",
        "        dec4_1 = torch.cat((dec4_1, enc4_1), dim=1) # Concatenate the encoder and decoder outputs\n",
        "        dec4_1 = self.decoder4(dec4_1)\n",
        "        dec3_1 = self.upconv3(dec4_1)\n",
        "        dec3_1 = torch.cat((dec3_1, enc3_1), dim=1)\n",
        "        dec3_1 = self.decoder3(dec3_1)\n",
        "        dec2_1 = self.upconv2(dec3_1)\n",
        "        dec2_1 = torch.cat((dec2_1, enc2_1), dim=1)\n",
        "        dec2_1 = self.decoder2(dec2_1)\n",
        "        dec1_1 = self.upconv1(dec2_1)\n",
        "        dec1_1 = torch.cat((dec1_1, enc1_1), dim=1)\n",
        "        dec1_1 = self.decoder1(dec1_1)\n",
        "\n",
        "        # UNet on x2\n",
        "        enc1_2 = self.encoder1(x2)\n",
        "        enc2_2 = self.encoder2(self.pool1(enc1_2))\n",
        "        enc3_2 = self.encoder3(self.pool2(enc2_2))\n",
        "        enc4_2 = self.encoder4(self.pool3(enc3_2))\n",
        "\n",
        "        bottleneck_2 = self.bottleneck(self.pool4(enc4_2))\n",
        "\n",
        "        dec4_2 = self.upconv4(bottleneck_2)\n",
        "        dec4_2 = torch.cat((dec4_2, enc4_2), dim=1)\n",
        "        dec4_2 = self.decoder4(dec4_2)\n",
        "        dec3_2 = self.upconv3(dec4_2)\n",
        "        dec3_2 = torch.cat((dec3_2, enc3_2), dim=1)\n",
        "        dec3_2 = self.decoder3(dec3_2)\n",
        "        dec2_2 = self.upconv2(dec3_2)\n",
        "        dec2_2 = torch.cat((dec2_2, enc2_2), dim=1)\n",
        "        dec2_2 = self.decoder2(dec2_2)\n",
        "        dec1_2 = self.upconv1(dec2_2)\n",
        "        dec1_2 = torch.cat((dec1_2, enc1_2), dim=1)\n",
        "        dec1_2 = self.decoder1(dec1_2)\n",
        "\n",
        "        # Siamese (calculating differences between the pre and post images)\n",
        "        dec1_c = bottleneck_2 - bottleneck_1\n",
        "\n",
        "        dec1_c = self.upconv4_c(dec1_c) # features * 16 -> features * 8\n",
        "        diff_2 = enc4_2 - enc4_1 # features * 16 -> features * 8\n",
        "        dec2_c = torch.cat((diff_2, dec1_c), dim=1) # Combine differences and decoder output\n",
        "        dec2_c = self.conv4_c(dec2_c)\n",
        "\n",
        "        dec2_c = self.upconv3_c(dec2_c) # 512->256\n",
        "        diff_3 = enc3_2 - enc3_1\n",
        "        dec3_c = torch.cat((diff_3, dec2_c), dim=1) # Combine differences and decoder output\n",
        "        dec3_c = self.conv3_c(dec3_c)\n",
        "\n",
        "        dec3_c = self.upconv2_c(dec3_c) #512->256\n",
        "        diff_4 = enc2_2 - enc2_1\n",
        "        dec4_c = torch.cat((diff_4, dec3_c), dim=1) #\n",
        "        dec4_c = self.conv2_c(dec4_c)\n",
        "\n",
        "        dec4_c = self.upconv1_c(dec4_c)\n",
        "        diff_5 = enc1_2 - enc1_1\n",
        "        dec5_c = torch.cat((diff_5, dec4_c), dim=1)\n",
        "        dec5_c = self.conv1_c(dec5_c)\n",
        "\n",
        "        return self.conv_s(dec1_1), self.conv_s(dec1_2), self.conv_c(dec5_c)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential( #Helper function to create the blocks used in both encoding and decoding phases.\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )"
      ],
      "metadata": {
        "id": "II8jzQXoxnQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Vi80ontLxnNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset Splits, Mean/Std Statistics, and Creating Datasets for Train, Validation, and Test Sets"
      ],
      "metadata": {
        "id": "knzC3B9DA4pN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def load_data(split_json_path, mean_std_json_path, base_path=\"shards\"):\n",
        "    # Load split definitions\n",
        "    with open(split_json_path, \"r\") as f:\n",
        "        raw_splits = json.load(f)  # Read the split information (train, val, test) for each scene\n",
        "    print(\"‚úÖ Loaded scenes:\", len(raw_splits)) # Print the number of scenes loaded from the split file\n",
        "\n",
        "    # Load mean/std statistics\n",
        "    with open(mean_std_json_path, \"r\") as f:\n",
        "        mean_std = json.load(f)  # Read the mean and std statistics for each tile\n",
        "    print(\"‚úÖ Loaded mean/std stats:\", len(mean_std))   # Print the number of mean/std entries\n",
        "\n",
        "    # Extract all .npz files available (without extension)\n",
        "    available_tiles = {\n",
        "        fname.replace(\".npz\", \"\") # Remove the .npz extension to get the tile ID\n",
        "        for fname in os.listdir(base_path)\n",
        "        if fname.endswith(\".npz\")\n",
        "    }\n",
        "\n",
        "    # Helper to filter and match with available npz files\n",
        "    def collect_ids(split_name):\n",
        "        ids = [] # Initialize the list to store matching tile IDs\n",
        "        for scene_id, splits in raw_splits.items():  # Iterate through the raw splits (train/val/test)\n",
        "            for tile_id in splits.get(split_name, []): # Get tile IDs for the specific split (train/val/test)\n",
        "                if tile_id in available_tiles and tile_id in mean_std:  # Check if tile exists and has mean/std stats\n",
        "                    ids.append(tile_id) # Add the matching tile ID to the list\n",
        "        return ids\n",
        "\n",
        "    # Collect tile IDs for each split (train, validation, test)\n",
        "    train_ids = collect_ids(\"train\")\n",
        "    val_ids = collect_ids(\"val\")\n",
        "    test_ids = collect_ids(\"test\")\n",
        "\n",
        "    print(f\"‚úÖ Final splits ‚Üí Train: {len(train_ids)}, Val: {len(val_ids)}, Test: {len(test_ids)}\")\n",
        "\n",
        "    # Create dataset objects for each split using the filtered tile IDs and mean/std statistics\n",
        "    train_ds = DamageDataset(train_ids, mean_std, transform=True, base_path=base_path) # Train dataset with augmentation\n",
        "    val_ds = DamageDataset(val_ids, mean_std, transform=False, base_path=base_path) # Validation dataset without augmentation\n",
        "    test_ds = DamageDataset(test_ids, mean_std, transform=False, base_path=base_path) # Test dataset without augmentation\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ],
      "metadata": {
        "id": "casFChDBxnLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Validation: Training Loop with Loss Calculation, Optimizer, and Saving Best Model"
      ],
      "metadata": {
        "id": "IBd2PCXEBsI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_ds,\n",
        "    val_ds,\n",
        "    device,\n",
        "    epochs=30,\n",
        "    batch_size=8,\n",
        "    save_dir=\"checkpoints\",\n",
        "    num_classes=5,\n",
        "    ignore_index=255,\n",
        "    early_stopping_patience=3\n",
        "):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    # Define loss function weights for segmentation and damage classification\n",
        "    weights_seg = torch.tensor([1.0, 15.0], dtype=torch.float32).to(device)\n",
        "    weights_dmg = torch.tensor([1.0, 2.11, 2.51, 9.51, 17.02], dtype=torch.float32).to(device)\n",
        "    # weights_dmg = torch.tensor([1.0, 35.0, 70.0, 150.0, 120.0], dtype=torch.float32).to(device)\n",
        "\n",
        "    # Using only CrossEntropyLoss\n",
        "    loss_seg_pre = torch.nn.CrossEntropyLoss(weight=weights_seg, ignore_index=ignore_index)\n",
        "    loss_seg_post = torch.nn.CrossEntropyLoss(weight=weights_seg, ignore_index=ignore_index)\n",
        "    loss_dmg = torch.nn.CrossEntropyLoss(weight=weights_dmg, ignore_index=ignore_index)\n",
        "\n",
        "    loss_weights = [0.0, 0.0, 1.0]\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\")):\n",
        "            pre = batch[\"pre_image\"].to(device)\n",
        "            post = batch[\"post_image\"].to(device)\n",
        "            bld_mask = batch[\"building_mask\"].to(device)\n",
        "            dmg_mask = batch[\"damage_mask\"].to(device)\n",
        "\n",
        "            dmg_mask = torch.clamp(dmg_mask, 0, num_classes - 1) # Clamp damage mask to valid class indices\n",
        "\n",
        "            optimizer.zero_grad() # Zero the gradients before the backward pass\n",
        "            out_pre, out_post, out_dmg = model(pre, post) # Forward pass through the model\n",
        "\n",
        "            # Calculate building mask predictions\n",
        "            with torch.no_grad():\n",
        "                pred_building = torch.argmax(out_pre.softmax(dim=1), dim=1)\n",
        "\n",
        "            # Apply mask to damage predictions (consider only damage where building is present)\n",
        "            for c in range(out_dmg.shape[1]):\n",
        "                out_dmg[:, c] *= (pred_building == 1)\n",
        "\n",
        "            # Calculate the individual losses for segmentation and damage classification\n",
        "            try:\n",
        "                loss_pre = loss_seg_pre(out_pre, bld_mask)\n",
        "                loss_post = loss_seg_post(out_post, bld_mask)\n",
        "                loss_damage = loss_dmg(out_dmg, dmg_mask)\n",
        "\n",
        "                 # Combine the losses with appropriate weights\n",
        "                loss = loss_weights[0]*loss_pre + loss_weights[1]*loss_post + loss_weights[2]*loss_damage\n",
        "                loss.backward() # Backward pass\n",
        "                optimizer.step() # Optimizer step\n",
        "                total_train_loss += loss.item() # Add loss to total training loss\n",
        "            except RuntimeError as e:\n",
        "                print(\"‚ùå Runtime error during loss computation:\", e)\n",
        "                continue\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval() # Set the model to evaluation mode\n",
        "        total_val_loss = 0 # Initialize total validation loss\n",
        "\n",
        "        with torch.no_grad(): # No gradient calculation during validation\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\"):\n",
        "               # Load the validation data\n",
        "                pre = batch[\"pre_image\"].to(device)\n",
        "                post = batch[\"post_image\"].to(device)\n",
        "                bld_mask = batch[\"building_mask\"].to(device)\n",
        "                dmg_mask = batch[\"damage_mask\"].to(device)\n",
        "\n",
        "                dmg_mask = torch.clamp(dmg_mask, 0, num_classes - 1) # Clamp damage mask to valid class indices\n",
        "                out_pre, out_post, out_dmg = model(pre, post)\n",
        "\n",
        "                 # Apply the building mask logic for damage predictions\n",
        "                pred_building = torch.argmax(out_pre.softmax(dim=1), dim=1)\n",
        "                for c in range(out_dmg.shape[1]):\n",
        "                    out_dmg[:, c] *= (pred_building == 1)\n",
        "\n",
        "                 # Calculate the validation losses\n",
        "                loss_pre = loss_seg_pre(out_pre, bld_mask)\n",
        "                loss_post = loss_seg_post(out_post, bld_mask)\n",
        "                loss_damage = loss_dmg(out_dmg, dmg_mask)\n",
        "\n",
        "                # Combine the validation losses\n",
        "                loss = loss_weights[0]*loss_pre + loss_weights[1]*loss_post + loss_weights[2]*loss_damage\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        # Calculate and print average validation loss for the epoch\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Epoch {epoch+1}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save the best model (with lowest validation loss)\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
        "            print(f\"‚úÖ Best model saved at epoch {epoch+1} with val loss {avg_val_loss:.4f}\")\n",
        "        #     patience_counter = 0\n",
        "        # else:\n",
        "        #     patience_counter += 1\n",
        "        #     print(f\"‚è≥ No improvement. Patience counter: {patience_counter}/{early_stopping_patience}\")\n",
        "        #     if patience_counter >= early_stopping_patience:\n",
        "        #         print(\"‚èπÔ∏è Early stopping triggered.\")\n",
        "        #         break\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(save_dir, \"loss_curve_ce_only.png\"))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "G-TktKLmrpYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# del model\n",
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "-y89NROB0yuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SiamUnet().float().to(device)\n",
        "\n",
        "train_ds, val_ds, test_ds = load_data(\n",
        "    split_json_path=\"final_splits_sliced.json\",\n",
        "    mean_std_json_path=\"mean_stddev_tiles.json\",\n",
        "    base_path=\"/content/shards\"\n",
        ")\n",
        "\n",
        "train_model(\n",
        "    model=model,\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    device=device,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    save_dir=\"/content/drive/MyDrive/691_Team4_Outputs/checkpoints\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "2inxIoAnzdTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Visualization: Generating Classification Report, Confusion Matrix, and IoU Scores"
      ],
      "metadata": {
        "id": "QoyFshLWC9hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay,\n",
        "    jaccard_score,\n",
        "    accuracy_score\n",
        ")\n",
        "\n",
        "def denormalize(img_tensor, mean, std):\n",
        "    \"\"\"Reverse normalization for visualization\"\"\"\n",
        "    mean = torch.tensor(mean).view(-1, 1, 1).to(img_tensor.device)\n",
        "    std = torch.tensor(std).view(-1, 1, 1).to(img_tensor.device)\n",
        "    return img_tensor * std + mean\n",
        "\n",
        "def visualize_prediction(pre, post, pred, mask, save_path):\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(16, 4)) # Create 4 subplots for each image\n",
        "\n",
        "    axs[0].imshow(pre.permute(1, 2, 0).cpu().numpy()) # Display pre-disaster image\n",
        "    axs[0].set_title(\"Pre-disaster\")\n",
        "\n",
        "    axs[1].imshow(post.permute(1, 2, 0).cpu().numpy()) # Display post-disaster image\n",
        "    axs[1].set_title(\"Post-disaster\")\n",
        "\n",
        "    axs[2].imshow(pred.cpu().numpy(), cmap=\"viridis\", vmin=0, vmax=4)  # Display predicted mask\n",
        "    axs[2].set_title(\"Predicted Mask\")\n",
        "\n",
        "    axs[3].imshow(mask.cpu().numpy(), cmap=\"viridis\", vmin=0, vmax=4) # Display ground truth mask\n",
        "    axs[3].set_title(\"Ground Truth\")\n",
        "\n",
        "    for ax in axs:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "def evaluate_model(model, test_loader, device, save_dir=\"checkpoints\", num_classes=5, ignore_index=255):\n",
        "   # Load the best model from the specified directory\n",
        "    model_path = os.path.join(save_dir, \"best_model.pth\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Best model not found at: {model_path}\")\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_masks = []\n",
        "\n",
        "    vis_dir = os.path.join(save_dir, \"test_visuals\")\n",
        "    os.makedirs(vis_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            pre = batch[\"pre_image\"].to(device).float() # Load pre-disaster image\n",
        "            post = batch[\"post_image\"].to(device).float() # Load post-disaster image\n",
        "            mask = batch[\"damage_mask\"].to(device) # Load ground truth damage mask\n",
        "\n",
        "            _, _, logits = model(pre, post) # Get the model predictions\n",
        "            preds = torch.argmax(logits, dim=1) # Get the predicted class for each pixel\n",
        "\n",
        "            # Append predictions and masks to lists\n",
        "            for p, t in zip(preds, mask):\n",
        "                all_preds.append(p.flatten().cpu().numpy())\n",
        "                all_masks.append(t.flatten().cpu().numpy())\n",
        "\n",
        "            # Denormalize for visualization\n",
        "            if i < 5:\n",
        "                pre_vis = denormalize(pre[0], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                post_vis = denormalize(post[0], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "                visualize_prediction(pre_vis, post_vis, preds[0], mask[0], os.path.join(vis_dir, f\"sample_{i}.png\"))\n",
        "\n",
        "    # Flatten the lists for metrics calculation\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_masks = np.concatenate(all_masks)\n",
        "\n",
        "    valid = all_masks != ignore_index # Exclude invalid pixels (ignore_index)\n",
        "    y_true = all_masks[valid] # Ground truth\n",
        "    y_pred = all_preds[valid] # Predictions\n",
        "\n",
        "    # üìÑ Classification Report\n",
        "    cls_report = classification_report(y_true, y_pred, digits=4, zero_division=0)\n",
        "    print(\"‚úÖ Classification Report:\")\n",
        "    print(cls_report)\n",
        "    with open(os.path.join(save_dir, \"classification_report.txt\"), \"w\") as f:\n",
        "        f.write(cls_report)\n",
        "\n",
        "    # üìä Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes))) # Compute confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[f\"Class {i}\" for i in range(num_classes)])\n",
        "    disp.plot(cmap=\"Blues\", xticks_rotation=45)\n",
        "    plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"))\n",
        "    np.save(os.path.join(save_dir, \"confusion_matrix.npy\"), cm)\n",
        "    plt.close()\n",
        "\n",
        "    # üìà IoU per class\n",
        "    ious = jaccard_score(y_true, y_pred, average=None, labels=list(range(num_classes)), zero_division=0)\n",
        "    iou_lines = [f\"Class {i}: {iou:.4f}\" for i, iou in enumerate(ious)]\n",
        "    mean_iou = np.mean(ious)\n",
        "    print(\"üìà Mean IoU per class:\")\n",
        "    print(\"\\n\".join(iou_lines))\n",
        "    print(f\"üîÅ Mean IoU: {mean_iou:.4f}\")\n",
        "\n",
        "    with open(os.path.join(save_dir, \"ious_per_class.txt\"), \"w\") as f:\n",
        "        f.write(\"\\n\".join(iou_lines))\n",
        "        f.write(f\"\\nMean IoU: {mean_iou:.4f}\")\n",
        "\n",
        "    # ‚úÖ Overall Accuracy\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    with open(os.path.join(save_dir, \"accuracy.txt\"), \"w\") as f:\n",
        "        f.write(f\"Overall Accuracy: {acc:.4f}\")\n",
        "    print(f\"‚úÖ Overall Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "id": "K2ahZBLgzdOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Test Data and Evaluating the Model: Prediction, Metrics, and Saving Results"
      ],
      "metadata": {
        "id": "eFRcWtOmGiF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# Creating a DataLoader for the test dataset with specified parameters\n",
        "test_loader = DataLoader(\n",
        "    test_ds,              # your test dataset\n",
        "    batch_size=8,         # reasonable batch size for evaluation\n",
        "    shuffle=False,        # no shuffling for test evaluation\n",
        "    num_workers=4,        # parallel loading\n",
        "    pin_memory=True       # improves performance on CUDA\n",
        ")\n",
        "\n",
        "evaluate_model(\n",
        "    model=model,  # your trained SiamUnet model\n",
        "    test_loader=test_loader,\n",
        "    device=device,\n",
        "    save_dir=\"/content/drive/MyDrive/691_Team4_Outputs/checkpoints\",  # where to save results\n",
        "    num_classes=5,\n",
        "    ignore_index=255\n",
        ")"
      ],
      "metadata": {
        "id": "Y3wWjeJDzdMB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}