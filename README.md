# Post_Disaster_Building_Damage_Assesment_Using_Deep_Learning
Disasters, both natural and man-made, pose critical challenges to emergency response systems and resource allocation strategies. As the frequency and severity of natural disasters continue to rise, developing resilient systems capable of anticipating, absorbing, and recovering from these disruptions has become a national priority in the United States. Resilience analytics—an emerging field that combines predictive modeling with geospatial analysis—provides a powerful framework for enhancing disaster preparedness and response. Building on advances in big data and artificial intelligence (AI), recent research has shown that data-driven approaches can significantly improve situational awareness and decision-making in disaster risk management (Sarker et al., 2020; Ghaffarian & Taghikhah, 2023). This project investigates the application of deep learning and satellite imagery for rapid and accurate assessment of building damage following disasters, utilizing the xView2 dataset which includes events like windstorms, floods, and tsunamis. Initially, our research questions focused on integrating heterogeneous data sources–including social media, IoT sensor data, and emergency service logs—to support real-time response and public sentiment analysis. However, due to the time-intensive nature of multi-source integration and the complexity of image classification, I refined our scope to focus exclusively on geospatial data.

The research concentrates on the direct analysis of pre- and post-disaster satellite imagery. To this end, I developed and evaluated a system based on a Siamese UNet architecture. This model is specifically designed to analyze paired pre- and post-disaster imagery to identify and classify structural damage at the building level through semantic segmentation. This shift allowed to develop a more focused and technically rigorous approach to structural damage assessment while still pursuing the broader goal of enhancing disaster preparedness through real-time predictive analytics.

# Exploratory Data Analysis
To better understand the characteristics and patterns within the xView2 dataset, I conducted an exploratory data analysis (EDA) focused on buildings affected by windstorms. This involved visualizing building damage levels, analyzing geometric properties, and mapping spatial distributions. First, the distribution of damage grades across all annotated buildings was analyzed. The bar chart revealed a heavy skew toward the "no damage" category (43,746 buildings), with fewer instances labeled as "minor damage" (20,742 buildings), "major damage" (17,463 buildings), and “destroyed” (4,3610 buildings), and a subset marked as "unknown" (2,5778 buildings) as a result of annotation gaps or unclear imagery. This class imbalance is a notable characteristic of the dataset and has direct implications for model training, requiring appropriate strategies, such as class weighting or oversampling, to avoid bias in predictions. 
![EDA pic](https://github.com/user-attachments/assets/6bc146f0-a0d6-4dd1-af35-c92ac98348a1)

# Modeling and Results:
This project applies a deep learning approach to analyze satellite imagery captured before and after natural disasters, aiming to automatically detect buildings and classify the level of damage sustained. The system comprises several stages: preparing the input data, generating pixel-wise labels, dividing the data into manageable units, and designing a neural network capable of performing both segmentation and classification.

We process satellite imagery from .tif format by extracting the RGB channels to create standard visual images. To handle variations in lighting conditions across different satellite captures, we apply normalization techniques that adjust pixel values to a consistent range. This normalization step is crucial for the neural network to learn effectively without being influenced by brightness inconsistencies.

The project uses structured JSON files containing building outlines and damage classifications to create training labels. We convert these vector-based annotations into pixel-level masks where each building pixel receives a numerical code representing its damage category (ranging from undamaged to completely destroyed). This transformation creates ground truth data that our model can learn from.

![Pre and Post patches](https://github.com/user-attachments/assets/8b9bd13f-b31a-4c9e-9baa-5d0719fadf46)

Since satellite images are large and complex, we divide each scene into smaller square sections or "tiles" measuring 256 by 256 pixels. This step simplifies the learning process and allows the model to focus on localized features. Only patches that contain labeled buildings are kept to ensure training data remains relevant. Each patch contains four components such as the pre-disaster image, the post-disaster image, the building mask and the damage mask. These are saved in compressed files to speed up loading during training.

![Masking](https://github.com/user-attachments/assets/64bf2f04-2f09-40b0-bb24-af3f440dfedc)

The deep learning model used is a variant of the U-Net architecture, known as Siamese U-Net. It has two identical input paths, one for the pre-disaster image and one for the post-disaster image. Each path extracts features independently but using shared weights. These features are then combined to highlight changes in the image pair.

By combining the strengths of encoder-decoder networks with the ability to compare two inputs, this architecture is well-suited for change detection and damage assessment tasks. The model outputs binary segmentation mask from pre and post disaster images along with multi-class damage map that assigns each pixel a damage category.
The model is trained with two objectives in mind: identifying where buildings are located, and determining the extent of damage they have sustained. Two loss functions are used, one for segmentation and another for classification, and the total training loss is a weighted sum of both. This combined objective helps the model learn spatial structure and semantic meaning simultaneously.

Our training configuration includes the following carefully selected parameters:
Learning rate: 0.001 
Loss function: Cross Entropy Loss 
Optimizer: Adam 
Batch size: 32 
Epochs: 30

![Loss Convergence](https://github.com/user-attachments/assets/d2366a22-02b9-4d4d-9a16-f0402f76a53b)

The model undergoes training with these parameters while monitoring validation performance. We implement early stopping criteria based on validation metrics to select the optimal model version that generalizes best to unseen disaster events.
![Confusion matrix](https://github.com/user-attachments/assets/b4064d74-1d4b-40d5-bb64-e1b1f8b01682)

Overall, the results demonstrate the effectiveness of Siamese UNet architectures in identifying significant structural damage from satellite imagery, with robust performance for higher-severity damage categories. These findings establish a solid foundation for building real-time, scalable disaster response tools based on geospatial AI.
![Results](https://github.com/user-attachments/assets/33b26270-3ce4-4709-8f1e-760ab74fe26a)

# Future work:
Our findings demonstrate the potential of Siamese UNet architectures for rapidly assessing post-disaster building damage using satellite imagery. This approach effectively captures temporal changes between pre- and post-disaster scenes, offering a scalable alternative to manual assessments. However, our analysis is not without limitations. One key constraint was the semantic segmentation resolution, which lacked the precision needed to distinguish subtle damage patterns in dense urban areas. Enhancing pixel-level classification could address this by providing more fine-grained differentiation between damaged and undamaged structures. Additionally, background elements such as roads, vegetation, and water bodies sometimes confused the model, highlighting the need for improved filtering techniques. Future research could explore the integration of pre-segmentation methods or building footprint masks to ensure the model focuses on relevant areas. Another limitation involved inconsistencies in satellite image quality due to varying atmospheric conditions and sensor settings. We learned that preprocessing plays a crucial role in model reliability; with additional time, we would experiment with techniques such as brightness normalization, histogram equalization, or CLAHE to improve consistency across images. Finally, although we focused on windstorm-related damage in this project, the broader applicability of our method to other disasters is a strong direction for future research. Through this project, we gained valuable experience in balancing technical ambition with practical constraints. One important lesson was the need to narrow scope early when integrating multiple data sources becomes infeasible. Given more time, we would revisit our initial goal of multi-modal data fusion (combining satellite imagery with social media or sensor data) to enrich situational awareness and build more holistic disaster response tools.
